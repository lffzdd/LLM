"""
===========================================
ç¬¬ä¸‰è¯¾ï¼šå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ (Multinomial Naive Bayes)
===========================================

å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ç”¨äºå¤„ç†ç¦»æ•£ç‰¹å¾ï¼Œç‰¹åˆ«é€‚åˆæ–‡æœ¬åˆ†ç±»ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š
- å‡è®¾ç‰¹å¾æ˜¯è¯é¢‘æˆ–è¯è®¡æ•°
- æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾æœä»å¤šé¡¹å¼åˆ†å¸ƒ

å…³é”®å…¬å¼ï¼š
P(x_i|y=c) = (count(x_i, c) + Î±) / (count(c) + Î± Ã— |V|)

å…¶ä¸­ï¼š
- count(x_i, c)ï¼šç±»åˆ« c ä¸­ç‰¹å¾ i å‡ºç°çš„æ¬¡æ•°
- count(c)ï¼šç±»åˆ« c ä¸­æ‰€æœ‰ç‰¹å¾çš„æ€»è®¡æ•°
- Î±ï¼šå¹³æ»‘å‚æ•°ï¼ˆæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼‰
- |V|ï¼šç‰¹å¾è¯æ±‡è¡¨å¤§å°
"""

import numpy as np
from collections import Counter, defaultdict


class MultinomialNaiveBayes:
    """
    å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨

    é€‚ç”¨åœºæ™¯ï¼š
    - æ–‡æœ¬åˆ†ç±»ï¼ˆåƒåœ¾é‚®ä»¶æ£€æµ‹ã€æƒ…æ„Ÿåˆ†æç­‰ï¼‰
    - ç‰¹å¾æ˜¯è®¡æ•°æˆ–é¢‘ç‡
    """

    def __init__(self, alpha: float = 1.0):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨

        å‚æ•°:
            alpha: æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å‚æ•°ï¼ˆé»˜è®¤ä¸º 1.0ï¼‰
                   - Î± = 0ï¼šä¸å¹³æ»‘ï¼ˆå¯èƒ½å¯¼è‡´é›¶æ¦‚ç‡é—®é¢˜ï¼‰
                   - Î± = 1ï¼šæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
                   - Î± < 1ï¼šLidstone å¹³æ»‘
        """
        self.alpha = alpha
        self.classes = None
        self.class_priors = {}  # P(y)
        self.feature_probs = {}  # P(x_i|y)
        self.vocabulary_size = 0

    def fit(self, X: np.ndarray, y: np.ndarray):
        """
        è®­ç»ƒæ¨¡å‹

        å‚æ•°:
            X: è®­ç»ƒæ•°æ®ï¼ˆè¯é¢‘çŸ©é˜µï¼‰ï¼Œå½¢çŠ¶ (n_samples, n_features)
            y: æ ‡ç­¾ï¼Œå½¢çŠ¶ (n_samples,)
        """
        n_samples, n_features = X.shape
        self.classes = np.unique(y) # uniqueæ–¹æ³•è¿”å›å”¯ä¸€çš„å…ƒç´ ,ä¾‹å¦‚[1,1,2,2,3]è¿”å›[1,2,3],å‡è®¾yæ˜¯[0,0,1,0,1,1],åˆ™uniqueè¿”å›[0,1]
        self.vocabulary_size = n_features

        print("=" * 50)
        print("è®­ç»ƒå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨")
        print("=" * 50)
        print(f"æ ·æœ¬æ•°: {n_samples}")
        print(f"ç‰¹å¾æ•°ï¼ˆè¯æ±‡é‡ï¼‰: {n_features}")
        print(f"ç±»åˆ«: {self.classes}")
        print(f"å¹³æ»‘å‚æ•° Î±: {self.alpha}")
        print()

        for c in self.classes:
            # ç­›é€‰è¯¥ç±»åˆ«çš„æ ·æœ¬
            X_c = X[y == c]

            # è®¡ç®—å…ˆéªŒæ¦‚ç‡
            self.class_priors[c] = len(X_c) / n_samples

            # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ¡ä»¶æ¦‚ç‡
            # P(x_i|y=c) = (count(x_i, c) + Î±) / (count(c) + Î± Ã— |V|)
            feature_counts = np.sum(X_c, axis=0)  # ç±»åˆ«cä¸­å‡ºç°æ¯ä¸ªç‰¹å¾çš„æ¬¡æ•°,å¦‚[0,1,2,3]è¡¨ç¤ºç±»åˆ«cä¸­å‡ºç°0æ¬¡ç‰¹å¾0,1æ¬¡ç‰¹å¾1,2æ¬¡ç‰¹å¾2,3æ¬¡ç‰¹å¾3
            total_count = np.sum(feature_counts)  # æ‰€æœ‰ç‰¹å¾çš„æ€»è®¡æ•°

            # åº”ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
            # å‡è®¾æ¯ä¸ªç‰¹å¾è‡³å°‘å‡ºç°äº†alphaæ¬¡,é‚£ä¹ˆæ€»ç‰¹å¾æ¬¡æ•°è‡³å°‘å‡ºç°äº†n_features*alphaæ¬¡
            # ç±»åˆ«cä¸­æ¯ä¸ªç‰¹å¾çš„å‡ºç°é¢‘ç‡,ç”¨äºè¿ä¹˜è®¡ç®—ä¼¼ç„¶
            self.feature_probs[c] = (feature_counts + self.alpha) / (
                total_count + self.alpha * n_features
            )

            print(f"ç±»åˆ« {c}:")
            print(f"  æ ·æœ¬æ•°: {len(X_c)}")
            print(f"  å…ˆéªŒæ¦‚ç‡ P(y={c}) = {self.class_priors[c]:.4f}")
            print(f"  ç‰¹å¾æ€»è®¡æ•°: {total_count}")
            print()

        print("è®­ç»ƒå®Œæˆï¼\n")

    def _calculate_log_posterior(self, x: np.ndarray, c) -> float:
        """
        è®¡ç®—å¯¹æ•°åéªŒæ¦‚ç‡

        ä½¿ç”¨å¯¹æ•°æ¦‚ç‡é¿å…æ•°å€¼ä¸‹æº¢ï¼š
        log P(y=c|x) âˆ log P(y=c) + Î£ x_i Ã— log P(x_i|y=c)

        å‚æ•°:
            x: å•ä¸ªæ ·æœ¬çš„ç‰¹å¾å‘é‡
            c: ç±»åˆ«

        è¿”å›:
            å¯¹æ•°åéªŒæ¦‚ç‡ï¼ˆæœªå½’ä¸€åŒ–ï¼‰
        """
        log_prior = np.log(self.class_priors[c])
        log_likelihood = np.sum(x * np.log(self.feature_probs[c]))

        return log_prior + log_likelihood

    def predict_log_proba(self, X: np.ndarray) -> np.ndarray:
        """
        é¢„æµ‹å¯¹æ•°æ¦‚ç‡

        å‚æ•°:
            X: æµ‹è¯•æ•°æ®ï¼Œå½¢çŠ¶ (n_samples, n_features)

        è¿”å›:
            å¯¹æ•°æ¦‚ç‡çŸ©é˜µï¼Œå½¢çŠ¶ (n_samples, n_classes)
        """
        log_probas = []

        for x in X:
            log_posteriors = []
            for c in self.classes:
                log_posterior = self._calculate_log_posterior(x, c)
                log_posteriors.append(log_posterior)
            log_probas.append(log_posteriors)

        return np.array(log_probas)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        é¢„æµ‹æ¦‚ç‡

        å‚æ•°:
            X: æµ‹è¯•æ•°æ®ï¼Œå½¢çŠ¶ (n_samples, n_features)

        è¿”å›:
            æ¦‚ç‡çŸ©é˜µï¼Œå½¢çŠ¶ (n_samples, n_classes)
        """
        log_probas = self.predict_log_proba(X)

        # ä½¿ç”¨ log-sum-exp æŠ€å·§é¿å…æ•°å€¼é—®é¢˜
        # P(y=c|x) = exp(log P(c|x)) / Î£ exp(log P(c'|x))
        max_log_proba = np.max(log_probas, axis=1, keepdims=True)
        exp_log_probas = np.exp(log_probas - max_log_proba)
        probas = exp_log_probas / np.sum(exp_log_probas, axis=1, keepdims=True)

        return probas

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        é¢„æµ‹ç±»åˆ«

        å‚æ•°:
            X: æµ‹è¯•æ•°æ®ï¼Œå½¢çŠ¶ (n_samples, n_features)

        è¿”å›:
            é¢„æµ‹çš„ç±»åˆ«ï¼Œå½¢çŠ¶ (n_samples,)
        """
        log_probas = self.predict_log_proba(X)
        return self.classes[np.argmax(log_probas, axis=1)]


# ============================================
# è¾…åŠ©ç±»ï¼šç®€å•çš„æ–‡æœ¬å‘é‡åŒ–å™¨
# ============================================


class SimpleVectorizer:
    """
    ç®€å•çš„è¯è¢‹æ¨¡å‹å‘é‡åŒ–å™¨

    å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯é¢‘å‘é‡
    """

    def __init__(self):
        self.vocabulary = {}
        self.inv_vocabulary = {} # è¯æ±‡è¡¨çš„é€†æ˜ å°„

    def fit(self, texts: list):
        """
        æ„å»ºè¯æ±‡è¡¨

        å‚æ•°:
            texts: æ–‡æœ¬åˆ—è¡¨
        """
        word_set = set()
        for text in texts:
            words = text.lower().split()
            word_set.update(words)

        self.vocabulary = {word: i for i, word in enumerate(sorted(word_set))}
        self.inv_vocabulary = {i: word for word, i in self.vocabulary.items()}

        print(f"è¯æ±‡è¡¨å¤§å°: {len(self.vocabulary)}")
        print(f"è¯æ±‡è¡¨: {list(self.vocabulary.keys())[:10]}...")

    def transform(self, texts: list) -> np.ndarray:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯é¢‘å‘é‡

        å‚æ•°:
            texts: æ–‡æœ¬åˆ—è¡¨

        è¿”å›:
            è¯é¢‘çŸ©é˜µï¼Œå½¢çŠ¶ (n_samples, vocabulary_size)
        """
        vectors = []

        for text in texts:
            vector = np.zeros(len(self.vocabulary))
            words = text.lower().split()

            # ç»Ÿè®¡è¯é¢‘
            for word in words:
                if word in self.vocabulary:
                    vector[self.vocabulary[word]] += 1

            vectors.append(vector)

        return np.array(vectors)

    def fit_transform(self, texts: list) -> np.ndarray:
        """
        å…ˆ fit å† transform
        """
        self.fit(texts)
        return self.transform(texts)


# ============================================
# ç¤ºä¾‹ï¼šç®€å•æƒ…æ„Ÿåˆ†æ
# ============================================


def sentiment_example():
    """
    ç®€å•æƒ…æ„Ÿåˆ†æç¤ºä¾‹
    """
    # è®­ç»ƒæ•°æ®
    texts = [
        "I love this movie it is great",
        "This film is wonderful amazing",
        "Excellent movie best ever",
        "I love the acting great job",
        "This movie is terrible bad",
        "Worst film ever boring",
        "I hate this movie awful",
        "Terrible acting bad script",
    ]

    labels = np.array([1, 1, 1, 1, 0, 0, 0, 0])  # 1=æ­£é¢, 0=è´Ÿé¢

    print("=" * 50)
    print("ç®€å•æƒ…æ„Ÿåˆ†æç¤ºä¾‹")
    print("=" * 50)
    print("\nè®­ç»ƒæ•°æ®:")
    for text, label in zip(texts, labels):
        sentiment = "æ­£é¢ ğŸ˜Š" if label == 1 else "è´Ÿé¢ ğŸ˜"
        print(f"  [{sentiment}] {text}")
    print()

    # å‘é‡åŒ–
    vectorizer = SimpleVectorizer()
    X_train = vectorizer.fit_transform(texts)

    # è®­ç»ƒ
    clf = MultinomialNaiveBayes(alpha=1.0)
    clf.fit(X_train, labels)

    # æµ‹è¯•
    test_texts = [
        "this movie is great love it",
        "terrible movie I hate it",
        "the film is wonderful",
    ]

    X_test = vectorizer.transform(test_texts)
    predictions = clf.predict(X_test)
    probas = clf.predict_proba(X_test)

    print("=" * 50)
    print("é¢„æµ‹ç»“æœ")
    print("=" * 50)

    for text, pred, proba in zip(test_texts, predictions, probas):
        sentiment = "æ­£é¢ ğŸ˜Š" if pred == 1 else "è´Ÿé¢ ğŸ˜"
        print(f'\næ–‡æœ¬: "{text}"')
        print(f"é¢„æµ‹: {sentiment}")
        print(f"æ¦‚ç‡: è´Ÿé¢={proba[0]:.4f}, æ­£é¢={proba[1]:.4f}")


# ============================================
# è¯¦ç»†è§£é‡Šæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
# ============================================


def explain_laplace_smoothing():
    """
    è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
    """
    explanation = """
    ============================================
    ä¸ºä»€ä¹ˆéœ€è¦æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼Ÿ
    ============================================
    
    é—®é¢˜ï¼šé›¶æ¦‚ç‡é—®é¢˜
    
    å‡è®¾è®­ç»ƒæ•°æ®ï¼š
    - æ­£é¢è¯„è®ºä¸­æ²¡æœ‰å‡ºç°è¿‡ "terrible" è¿™ä¸ªè¯
    - æµ‹è¯•æ—¶é‡åˆ°: "This is terrible"
    
    å¦‚æœä¸å¹³æ»‘ï¼š
    P("terrible"|æ­£é¢) = 0
    P(æ­£é¢|æ–‡æœ¬) = P(æ­£é¢) Ã— ... Ã— P("terrible"|æ­£é¢) Ã— ... = 0
    
    æ— è®ºå…¶ä»–è¯æ®å¤šå¼ºï¼Œç»“æœéƒ½æ˜¯ 0ï¼è¿™æ˜¾ç„¶ä¸åˆç†ã€‚
    
    è§£å†³æ–¹æ¡ˆï¼šæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
    
    P(x_i|y=c) = (count(x_i, c) + Î±) / (count(c) + Î± Ã— |V|)
    
    - ç»™æ¯ä¸ªè¯åŠ ä¸Šä¸€ä¸ªå°çš„è®¡æ•° Î±ï¼ˆé€šå¸¸ Î±=1ï¼‰
    - åˆ†æ¯ä¹Ÿç›¸åº”è°ƒæ•´ï¼Œä¿è¯æ¦‚ç‡å’Œä¸º 1
    
    æ•ˆæœï¼š
    - æ²¡è§è¿‡çš„è¯ä¸ä¼šå¯¼è‡´é›¶æ¦‚ç‡
    - ç›¸å½“äºå‡è®¾æ¯ä¸ªè¯è‡³å°‘å‡ºç°è¿‡ Î± æ¬¡
    ============================================
    """
    print(explanation)

    # æ•°å€¼ç¤ºä¾‹
    print("æ•°å€¼ç¤ºä¾‹:")
    print("-" * 50)

    # å‡è®¾è¯æ±‡è¡¨å¤§å°ä¸º 10ï¼ŒæŸä¸ªç±»åˆ«çš„æ€»è¯æ•°ä¸º 100
    vocab_size = 10
    total_count = 100

    # æŸä¸ªè¯åœ¨è¯¥ç±»åˆ«ä¸­å‡ºç° 5 æ¬¡
    word_count = 5

    # å¦ä¸€ä¸ªè¯æ²¡å‡ºç°è¿‡
    unseen_word_count = 0

    # ä¸ä½¿ç”¨å¹³æ»‘
    print("\nä¸ä½¿ç”¨å¹³æ»‘ (Î±=0):")
    print(
        f"  P(è§è¿‡çš„è¯|ç±»åˆ«) = {word_count}/{total_count} = {word_count / total_count:.4f}"
    )
    print(
        f"  P(æ²¡è§è¿‡çš„è¯|ç±»åˆ«) = {unseen_word_count}/{total_count} = 0.0000 âŒ é—®é¢˜ï¼"
    )

    # ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
    alpha = 1
    print(f"\nä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ (Î±={alpha}):")
    smoothed_total = total_count + alpha * vocab_size
    print(
        f"  P(è§è¿‡çš„è¯|ç±»åˆ«) = ({word_count}+{alpha})/({total_count}+{alpha}Ã—{vocab_size}) = {(word_count + alpha) / smoothed_total:.4f}"
    )
    print(
        f"  P(æ²¡è§è¿‡çš„è¯|ç±»åˆ«) = ({unseen_word_count}+{alpha})/({total_count}+{alpha}Ã—{vocab_size}) = {(unseen_word_count + alpha) / smoothed_total:.4f} âœ“ ä¸å†æ˜¯é›¶ï¼"
    )


# ============================================
# ä¸»ç¨‹åº
# ============================================

if __name__ == "__main__":
    print("\n" + "ğŸ“ " * 20 + "\n")
    print("ç¬¬ä¸‰è¯¾ï¼šå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨")
    print("\n" + "ğŸ“ " * 20 + "\n")

    # è§£é‡Šæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
    explain_laplace_smoothing()

    print("\n")

    # æƒ…æ„Ÿåˆ†æç¤ºä¾‹
    sentiment_example()

    print("\n" + "=" * 50)
    print("ä¸‹ä¸€æ­¥ï¼šè¿è¡Œ 04_text_classification.py è¿›è¡Œå®æˆ˜ç»ƒä¹ ")
    print("=" * 50)
