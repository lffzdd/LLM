"""
===========================================
ç¬¬äºŒè¯¾ï¼šé«˜æ–¯æœ´ç´ è´å¶æ–¯ (Gaussian Naive Bayes)
===========================================

é«˜æ–¯æœ´ç´ è´å¶æ–¯ç”¨äºå¤„ç†è¿ç»­ç‰¹å¾ã€‚

æ ¸å¿ƒå‡è®¾ï¼š
1. ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼ˆæœ´ç´ å‡è®¾ï¼‰
2. æ¯ä¸ªç‰¹å¾åœ¨ç»™å®šç±»åˆ«ä¸‹æœä»é«˜æ–¯ï¼ˆæ­£æ€ï¼‰åˆ†å¸ƒ

é«˜æ–¯åˆ†å¸ƒå…¬å¼ï¼š
P(x|Î¼,Ïƒ) = (1/âˆš(2Ï€ÏƒÂ²)) Ã— exp(-(x-Î¼)Â²/(2ÏƒÂ²))

å…¶ä¸­ï¼š
- Î¼ (mu)ï¼šå‡å€¼
- Ïƒ (sigma)ï¼šæ ‡å‡†å·®
"""

import numpy as np
from collections import defaultdict


class GaussianNaiveBayes:
    """
    é«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨

    é€‚ç”¨åœºæ™¯ï¼š
    - ç‰¹å¾æ˜¯è¿ç»­å€¼ï¼ˆå¦‚èº«é«˜ã€ä½“é‡ã€æ¸©åº¦ç­‰ï¼‰
    - å‡è®¾ç‰¹å¾æœä»æ­£æ€åˆ†å¸ƒ
    """

    def __init__(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.classes = None  # æ‰€æœ‰ç±»åˆ«
        self.class_priors = {}  # å…ˆéªŒæ¦‚ç‡ P(y)
        self.means = {}  # æ¯ä¸ªç±»åˆ«æ¯ä¸ªç‰¹å¾çš„å‡å€¼
        self.variances = {}  # æ¯ä¸ªç±»åˆ«æ¯ä¸ªç‰¹å¾çš„æ–¹å·®

    def fit(self, X: np.ndarray, y: np.ndarray):
        """
        è®­ç»ƒæ¨¡å‹

        å‚æ•°:
            X: è®­ç»ƒæ•°æ®ï¼Œå½¢çŠ¶ (n_samples, n_features)
            y: æ ‡ç­¾ï¼Œå½¢çŠ¶ (n_samples,)
        """
        n_samples, n_features = X.shape
        self.classes = np.unique(y)

        print("=" * 50)
        print("è®­ç»ƒé«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨")
        print("=" * 50)
        print(f"æ ·æœ¬æ•°: {n_samples}")
        print(f"ç‰¹å¾æ•°: {n_features}")
        print(f"ç±»åˆ«: {self.classes}")
        print()

        # å¯¹æ¯ä¸ªç±»åˆ«
        for c in self.classes:
            # ç­›é€‰å‡ºå±äºè¯¥ç±»åˆ«çš„æ ·æœ¬
            """
            y == c ä¼šè¿”å›ä¸€ä¸ªå¸ƒå°”æ•°ç»„ï¼Œè¡¨ç¤º y ä¸­æ¯ä¸ªå…ƒç´ æ˜¯å¦ç­‰äº c
            X[y == c] ä¼šè¿”å› X ä¸­å¯¹åº”ä½ç½®ä¸º True çš„è¡Œ

            X = np.array([
                [5.1, 3.5],  # æ ·æœ¬0ï¼Œç±»åˆ«0
                [4.9, 3.0],  # æ ·æœ¬1ï¼Œç±»åˆ«0  
                [7.0, 3.2],  # æ ·æœ¬2ï¼Œç±»åˆ«1  â† ä¿ç•™
                [6.4, 3.2],  # æ ·æœ¬3ï¼Œç±»åˆ«1  â† ä¿ç•™
                [6.3, 3.3],  # æ ·æœ¬4ï¼Œç±»åˆ«2
            ])
            y = np.array([0, 0, 1, 1, 2])

            X_c = X[y == 1]
            # è¿”å›ï¼š
            # array([[7.0, 3.2],
            #        [6.4, 3.2]])
            """
            X_c = X[y == c]

            # è®¡ç®—å…ˆéªŒæ¦‚ç‡ P(y=c)
            # len(X_c) / n_samples è¡¨ç¤ºç±»åˆ« c çš„æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹
            self.class_priors[c] = len(X_c) / n_samples

            # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®
            """
            axis=0 è¡¨ç¤ºæŒ‰ç¬¬0è½´ï¼ˆè¡Œï¼‰è®¡ç®—

            array([[7.0, 3.2],
                   [6.4, 3.2]])
            ->
            mean:array([6.7, 3.2])  # å³ [(7.0+6.4)/2, (3.2+3.2)/2]
            var:array([0.125, 0.0])  # å³ [(7.0-6.7)^2/2, (3.2-3.2)^2/2]

            ç±»åˆ«cä¸­,ç¬¬0ä¸ªç‰¹å¾çš„å‡å€¼ä¸º6.7,ç¬¬1ä¸ªç‰¹å¾çš„å‡å€¼ä¸º3.2
            ç¬¬0ä¸ªç‰¹å¾çš„æ–¹å·®ä¸º0.125,ç¬¬1ä¸ªç‰¹å¾çš„æ–¹å·®ä¸º0.0
            """
            self.means[c] = np.mean(X_c, axis=0)
            self.variances[c] = np.var(X_c, axis=0)

            print(f"ç±»åˆ« {c}:")
            print(f"  æ ·æœ¬æ•°: {len(X_c)}")
            print(f"  å…ˆéªŒæ¦‚ç‡ P(y={c}) = {self.class_priors[c]:.4f}")
            print(f"  ç‰¹å¾å‡å€¼: {self.means[c]}")
            print(f"  ç‰¹å¾æ–¹å·®: {self.variances[c]}")
            print()

        print("è®­ç»ƒå®Œæˆï¼\n")

    def _gaussian_pdf(self, x: float, mean: float, var: float) -> float:
        """
        è®¡ç®—é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°å€¼

        P(x|Î¼,ÏƒÂ²) = (1/âˆš(2Ï€ÏƒÂ²)) Ã— exp(-(x-Î¼)Â²/(2ÏƒÂ²))

        å‚æ•°:
            x: è¾“å…¥å€¼
            mean: å‡å€¼ Î¼
            var: æ–¹å·® ÏƒÂ²

        è¿”å›:
            æ¦‚ç‡å¯†åº¦å€¼
        """
        # æ·»åŠ å°é‡é¿å…é™¤é›¶
        eps = 1e-10
        var = var + eps

        coefficient = 1 / np.sqrt(2 * np.pi * var)
        exponent = np.exp(-((x - mean) ** 2) / (2 * var))

        return coefficient * exponent

    def _calculate_posterior(self, x: np.ndarray, c) -> float:
        """
        è®¡ç®—åéªŒæ¦‚ç‡ï¼ˆæœªå½’ä¸€åŒ–ï¼‰

        P(y=c|x) âˆ P(y=c) Ã— âˆ P(x_i|y=c)

        å‚æ•°:
            x: å•ä¸ªæ ·æœ¬çš„ç‰¹å¾å‘é‡
            c: ç±»åˆ«

        è¿”å›:
            æœªå½’ä¸€åŒ–çš„åéªŒæ¦‚ç‡
        """
        # å…ˆéªŒæ¦‚ç‡
        prior = self.class_priors[c]

        # ä¼¼ç„¶ï¼šæ¯ä¸ªç‰¹å¾çš„é«˜æ–¯æ¦‚ç‡å¯†åº¦çš„ä¹˜ç§¯
        likelihood = 1.0
        for i, x_i in enumerate(x):
            pdf = self._gaussian_pdf(x_i, self.means[c][i], self.variances[c][i])
            likelihood *= pdf

        return prior * likelihood

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        é¢„æµ‹æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡

        å‚æ•°:
            X: æµ‹è¯•æ•°æ®ï¼Œå½¢çŠ¶ (n_samples, n_features)

        è¿”å›:
            æ¦‚ç‡çŸ©é˜µï¼Œå½¢çŠ¶ (n_samples, n_classes)
        """
        probas = []

        for x in X:
            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åéªŒæ¦‚ç‡,å³æ¯ä¸ªç±»åˆ«åœ¨xå‡ºç°æ—¶çš„æ¦‚ç‡
            posteriors = []
            for c in self.classes:
                posterior = self._calculate_posterior(x, c)
                posteriors.append(posterior)

            # å½’ä¸€åŒ–
            total = sum(posteriors)
            posteriors = [p / total for p in posteriors]
            probas.append(posteriors)

        return np.array(probas)

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        é¢„æµ‹ç±»åˆ«

        å‚æ•°:
            X: æµ‹è¯•æ•°æ®ï¼Œå½¢çŠ¶ (n_samples, n_features)

        è¿”å›:
            é¢„æµ‹çš„ç±»åˆ«ï¼Œå½¢çŠ¶ (n_samples,)
        """
        probas = self.predict_proba(X)
        # np.argmax(probas, axis=1) è¿”å›æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç±»åˆ«
        # ä¾‹å¦‚ï¼Œå¦‚æœ probas = [[0.2, 0.1, 0.7], [0.6, 0.2, 0.2]]ï¼Œåˆ™è¿”å› [2, 0]
        return self.classes[np.argmax(probas, axis=1)]


# ============================================
# ç¤ºä¾‹ï¼šä½¿ç”¨é¸¢å°¾èŠ±æ•°æ®é›†
# ============================================


def iris_example():
    """
    ä½¿ç”¨é¸¢å°¾èŠ±æ•°æ®é›†æ¼”ç¤ºé«˜æ–¯æœ´ç´ è´å¶æ–¯
    """
    # ç®€åŒ–ç‰ˆé¸¢å°¾èŠ±æ•°æ®ï¼ˆéƒ¨åˆ†æ ·æœ¬ï¼‰
    # ç‰¹å¾ï¼šèŠ±è¼é•¿åº¦, èŠ±è¼å®½åº¦, èŠ±ç“£é•¿åº¦, èŠ±ç“£å®½åº¦
    # ç±»åˆ«ï¼š0=setosa, 1=versicolor, 2=virginica

    # è®­ç»ƒæ•°æ®
    X_train = np.array(
        [
            # Setosa (ç±»åˆ« 0)
            [5.1, 3.5, 1.4, 0.2],
            [4.9, 3.0, 1.4, 0.2],
            [4.7, 3.2, 1.3, 0.2],
            [4.6, 3.1, 1.5, 0.2],
            [5.0, 3.6, 1.4, 0.2],
            # Versicolor (ç±»åˆ« 1)
            [7.0, 3.2, 4.7, 1.4],
            [6.4, 3.2, 4.5, 1.5],
            [6.9, 3.1, 4.9, 1.5],
            [5.5, 2.3, 4.0, 1.3],
            [6.5, 2.8, 4.6, 1.5],
            # Virginica (ç±»åˆ« 2)
            [6.3, 3.3, 6.0, 2.5],
            [5.8, 2.7, 5.1, 1.9],
            [7.1, 3.0, 5.9, 2.1],
            [6.3, 2.9, 5.6, 1.8],
            [6.5, 3.0, 5.8, 2.2],
        ]
    )

    y_train = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])

    # åˆ›å»ºå¹¶è®­ç»ƒåˆ†ç±»å™¨
    clf = GaussianNaiveBayes()
    clf.fit(X_train, y_train)

    # æµ‹è¯•æ•°æ®
    X_test = np.array(
        [
            [5.0, 3.4, 1.5, 0.2],  # åº”è¯¥æ˜¯ setosa
            [6.0, 2.9, 4.5, 1.5],  # åº”è¯¥æ˜¯ versicolor
            [6.7, 3.0, 5.5, 2.0],  # åº”è¯¥æ˜¯ virginica
        ]
    )

    print("=" * 50)
    print("é¢„æµ‹ç»“æœ")
    print("=" * 50)

    # é¢„æµ‹æ¦‚ç‡
    probas = clf.predict_proba(X_test)
    predictions = clf.predict(X_test)

    class_names = ["Setosa", "Versicolor", "Virginica"]

    for i, (x, pred, proba) in enumerate(zip(X_test, predictions, probas)):
        print(f"\næ ·æœ¬ {i + 1}: {x}")
        print(f"é¢„æµ‹ç±»åˆ«: {class_names[pred]}")
        print("å„ç±»åˆ«æ¦‚ç‡:")
        for j, name in enumerate(class_names):
            bar = "â–ˆ" * int(proba[j] * 20)
            print(f"  {name:12s}: {proba[j]:.4f} {bar}")


# ============================================
# ä¸ sklearn å¯¹æ¯”éªŒè¯
# ============================================


def compare_with_sklearn():
    """
    ä¸ sklearn çš„å®ç°å¯¹æ¯”ï¼ŒéªŒè¯æˆ‘ä»¬çš„å®ç°æ˜¯å¦æ­£ç¡®
    """
    try:
        from sklearn.naive_bayes import GaussianNB
        from sklearn.datasets import load_iris
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score

        print("\n" + "=" * 50)
        print("ä¸ sklearn å¯¹æ¯”éªŒè¯")
        print("=" * 50)

        # åŠ è½½å®Œæ•´çš„é¸¢å°¾èŠ±æ•°æ®é›†
        iris = load_iris()
        X, y = iris.data, iris.target

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        # æˆ‘ä»¬çš„å®ç°
        our_clf = GaussianNaiveBayes()
        our_clf.fit(X_train, y_train)
        our_predictions = our_clf.predict(X_test)
        our_accuracy = accuracy_score(y_test, our_predictions)

        # sklearn çš„å®ç°
        sklearn_clf = GaussianNB()
        sklearn_clf.fit(X_train, y_train)
        sklearn_predictions = sklearn_clf.predict(X_test)
        sklearn_accuracy = accuracy_score(y_test, sklearn_predictions)

        print(f"\næˆ‘ä»¬çš„å®ç°å‡†ç¡®ç‡: {our_accuracy:.4f}")
        print(f"sklearn å‡†ç¡®ç‡:   {sklearn_accuracy:.4f}")

        if np.allclose(our_accuracy, sklearn_accuracy, atol=0.01):
            print("\nâœ… éªŒè¯é€šè¿‡ï¼æˆ‘ä»¬çš„å®ç°ä¸ sklearn ç»“æœä¸€è‡´ï¼")
        else:
            print("\nâš ï¸ ç»“æœæœ‰å·®å¼‚ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ–¹å·®è®¡ç®—æ–¹å¼ç•¥æœ‰ä¸åŒ")

    except ImportError:
        print("\nğŸ’¡ æç¤ºï¼šå®‰è£… sklearn å¯ä»¥è¿›è¡Œå¯¹æ¯”éªŒè¯")
        print("   pip install scikit-learn")


# ============================================
# ä¸»ç¨‹åº
# ============================================

if __name__ == "__main__":
    print("\n" + "ğŸŒ¸ " * 20 + "\n")
    print("ç¬¬äºŒè¯¾ï¼šé«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨")
    print("\n" + "ğŸŒ¸ " * 20 + "\n")

    # è¿è¡Œé¸¢å°¾èŠ±ç¤ºä¾‹
    iris_example()

    # ä¸ sklearn å¯¹æ¯”
    compare_with_sklearn()

    print("\n" + "=" * 50)
    print("ä¸‹ä¸€æ­¥ï¼šè¿è¡Œ 03_multinomial_nb.py å­¦ä¹ å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯")
    print("=" * 50)
